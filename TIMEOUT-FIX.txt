# âš¡ QUICK FIX - Worker Timeout Issue

## ğŸ¯ What Happened:

Your crawl is WORKING! It found 61 real mentions from NewsAPI.

**But:** The worker timed out after 30 seconds because the crawl takes 2+ minutes with 14 search queries.

**Error:** `WORKER TIMEOUT (pid:39)` - Gunicorn killed the worker

**Solution:** Increase the worker timeout from 30 seconds to 300 seconds (5 minutes)

---

## âœ… IMMEDIATE FIX (2 Minutes):

### **Option 1: Just Fix Timeout (No Database Yet)**

1. **Go to your GitHub repo**
2. **Click on `Procfile`**
3. **Click pencil icon** (Edit)
4. **Replace content with:**
   ```
   web: gunicorn app:app --timeout 300 --workers 1
   ```
5. **Commit changes**
6. **Wait for redeploy** (2-3 min)
7. **Test crawl** - should complete now!

---

### **Option 2: Fix Timeout AND Add Database**

Use the new ZIP file with both fixes:

1. **Download webapp-database-FIXED.zip** (provided)
2. **Extract it**
3. **Create PostgreSQL database on Render:**
   - New + â†’ PostgreSQL
   - Name: utility-monitor-db
   - Plan: Free
   - Copy "Internal Database URL"
4. **Add DATABASE_URL to web service:**
   - Environment tab
   - Add: DATABASE_URL = (paste URL)
   - Save
5. **Upload files from ZIP to GitHub:**
   - Replace: Procfile, app.py, requirements.txt
   - Commit
6. **Wait for redeploy**
7. **Check logs for:** "Database initialized successfully"
8. **Test!**

---

## ğŸ” What the Logs Show:

**Good news:**
âœ… App started successfully
âœ… NewsAPI is working! Found 61 mentions
âœ… Crawl is progressing through phases
âœ… No Python errors

**The issue:**
âŒ Crawl took >30 seconds
âŒ Gunicorn default timeout = 30s
âŒ Worker killed mid-crawl

**The fix:**
âœ… Set timeout to 300s (5 minutes)
âœ… Crawls complete successfully

---

## ğŸ“Š Why Crawl Takes 2+ Minutes:

With 14 search queries:
- 14 queries to NewsAPI = ~30 seconds
- State PUC scraping = ~15 seconds  
- Legistar city councils = ~10 seconds
- Processing results = ~5 seconds
- **Total: ~60-90 seconds**

Plus network delays = **2-2.5 minutes total**

Default 30s timeout is too short!

---

## ğŸ¯ Your Choice:

### **Just Want It Working (No Database):**
1. Edit Procfile in GitHub
2. Change to: `web: gunicorn app:app --timeout 300 --workers 1`
3. Commit
4. Done!

### **Want Persistent Memory Too:**
1. Download webapp-database-FIXED.zip
2. Create PostgreSQL database
3. Add DATABASE_URL
4. Upload all files
5. Deploy
6. Done!

---

## âœ… After Fix:

**With just timeout fix:**
- âœ… Crawls complete successfully
- âŒ Data still disappears on refresh

**With timeout fix + database:**
- âœ… Crawls complete successfully
- âœ… Data persists forever
- âœ… Approved stays approved
- âœ… Deleted stays deleted

---

**Quickest fix right now: Just edit Procfile in GitHub and add `--timeout 300 --workers 1`**

That will get your crawls working immediately!
